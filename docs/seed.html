<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Frank Hellmann" />
  <title>Networks – Complex and Graph Neural</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Networks<br />
–<br />
Complex and Graph Neural</h1>
<p class="author">Frank Hellmann</p>
</header>
<h1 id="initial-observations">Initial observations</h1>
<p>We have a language clash: The graph in Graph Neural Networks (GNNs) is the network in Complex Networks. We will try not to use the word network in this Knowledge Repo. All Complex Networks and graphs we will consider are defined over an index set <span class="math inline">\(1 \dots n\)</span>.</p>
<h1 id="a-brief-taxonomy-of-functions-on-sets-and-graphs">A brief taxonomy of Functions on Sets and Graphs</h1>
<p>Neural Networks (NN) are function approximators. Set based Neural Networks approximate functions from a set of inputs, meaning there is no particular order of the inputs specified. We call such functions set based functions. Graph Neural Networks approximate functions from data associated to graphs, and again, there is no natural ordering for these data, we refer to the functions as graph based functions. Network Measures in Complex Network theory are graph based functions for graphs with no data associated to them. They might go to a single number, to the set of nodes, or the set of edges.</p>
<p>It’s instructive to briefly consider set based functions on sets where the elements of the set carry no data, or equivalently, functions that don’t depend on the data, in analogy to Network Measures. If such a function takes values in, e.g. the reals <span class="math inline">\(\mathbb{R}\)</span>, it should tell us something about the set aspects of the set of inputs we have. The only property of a set independent of its elements is its size, thus any set based function that does not depend on the data associated to the set elements, is a function of the size of the input set.</p>
<p>In contrast, for graph based functions, much more complex functions are neccessary.</p>
<h2 id="classification-through-permutations">Classification through permutations</h2>
<p>For all these functions, the order in which the inputs are presented does not matter. When we want to write a set based function taking values in the reals in the form <span class="math inline">\(f(x_1, x_2, \dots)\)</span> they are permutation invariant <span class="math display">\[\begin{aligned}
f(x_1, x_2, \dots) = f(x_{\sigma(1)}, x_{\sigma(2)}, \dots)\end{aligned}\]</span> where <span class="math inline">\(\sigma\)</span> denotes a permutation.</p>
<p>For a graph based function, we need to permute the edges and the vertices at the same time.</p>
<p><span class="math display">\[\begin{aligned}
f(x_1, x_2, \dots, e_{12}, \dots) = f(x_{\sigma(1)}, x_{\sigma(2)}, \dots, e_{\sigma(1),\sigma(2)})\end{aligned}\]</span></p>
<p>If the graph based function provides a set of data associated to the nodes or edges then the function has to be covariant under permutations:</p>
<p><span class="math display">\[\begin{aligned}
f_1(x_1, x_2, \dots) = f_{\sigma(1)}(x_{\sigma(1)}, x_{\sigma(2)}, \dots)\end{aligned}\]</span></p>
<p>Generally speaking the domain and image of the function, written as a function on ordinary spaces, carry a representation of some permutation group.</p>
<h2 id="tensor-towers">Tensor towers</h2>
<p>In the above the node features live in <span class="math inline">\(\mathbf{X}^n\)</span> and the edge features in <span class="math inline">\(\mathbf{X}_e^{n \times n} = \mathbf{X}_e^{2n}\)</span>. The permutation group <span class="math inline">\(S_n\)</span> acts naturally on these spaces by permuting indices in the tensor copies<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. More generally the permutation group also acts naturally on <span class="math inline">\(\mathbf{X}_k^{kn}\)</span> where <span class="math inline">\(\mathbf{X}_k\)</span> is some fixed base space.</p>
<p>We can take the direct sum of such spaces to obtain an entire tensor tower, <span class="math display">\[\begin{aligned}
\mathcal{X}= \bigoplus_{k=0}^{\infty} \mathbf{X}_k^{kn} \;.\end{aligned}\]</span> This space also transforms naturally under <span class="math inline">\(S_n\)</span>. It also has an interpretation as being the space that carries data on hypergraphs. Hypergraph Neural Networks have actually already been defined in <span class="citation" data-cites="feng2019hypergraph"></span>.</p>
<h1 id="general-structure-results-for-permutation-invariant-functions">General structure results for permutation invariant functions</h1>
<p>Theorems exist that guarantee that a permutation invariant function can be written as</p>
<p><span class="math display">\[\begin{aligned}
f(x_1, x_2, \dots) = f^* \left(\sum_i (f^{**}(x_i))\right)\end{aligned}\]</span></p>
<p>See theorem ?? <span class="citation" data-cites="zaheer2017deep"></span>. However this reperesentation might not be useful or meaningful, see e.g. the discussion in <span class="citation" data-cites="wagstaff2019limitations"></span>.</p>
<p>What is remarkable about this is that this representation theorem also allows us to compare functions on sets of different size, though the results of <span class="citation" data-cites="wagstaff2019limitations"></span> show that generalizations to larger sets is not guaranteed.</p>
<p><strong>Question:</strong> Are there such theorems for graph based functions already known?</p>
<h1 id="combining-functions">Combining functions</h1>
<p>Let’s stick to these representations of <span class="math inline">\(S_n\)</span> for now. We can denote the representations by noting the <span class="math inline">\(k\)</span> that occur in the expansion. <span class="math inline">\((1)\)</span> would act on a space <span class="math inline">\(\mathbf{X}^n\)</span>, <span class="math inline">\((1,2)\)</span> on <span class="math inline">\(\mathbf{X}_1^n \oplus \mathbf{X}_2^{2n}\)</span> and so on. We can then denote for a function what representation it’s domain and image carry. For example the function that counts the total edge weight <span class="math inline">\(\sum_{i,j} e_{i,j}\)</span> is of the form <span class="math inline">\((2) \rightarrow (0)\)</span>.</p>
<p>A set based function <span class="math inline">\(f\)</span> is of the form <span class="math inline">\((1) \rightarrow (0)\)</span>. A node wise network measure on a graph in the sense of Complex Networks is <span class="math inline">\((2) \rightarrow (1)\)</span>.</p>
<p>Functions can be combined in the obvious way, give a network measure <span class="math inline">\(C\)</span> and a set based neural network <span class="math inline">\(\mathcal{NN}\)</span> we can obtain a function on graphs by first applying the network measure and then the neural network.</p>
<p>More interestingly, we can also use set based functions in the other direction to lift. Given set based functions <span class="math inline">\(f: (1) \rightarrow (0)\)</span> and <span class="math inline">\(g: (1) \rightarrow (0)\)</span> on on <span class="math inline">\(S_{n}\)</span> we can create a set consisting of <span class="math inline">\(g_i = g(\{x_{ij}\}_{j})\)</span> that transforms naturally under <span class="math inline">\(S_n\)</span>. Calling this set <span class="math inline">\(\{g\}\)</span> we can then apply <span class="math inline">\(f\)</span> to it</p>
<p><span class="math display">\[f(\{g(x_{ij})\}) : (2) \rightarrow (0)\]</span></p>
<p>As an example, let the edge weights be the adjacency matrix <span class="math inline">\(A_{ij}\)</span>, take <span class="math inline">\(g\)</span> to be the sum function and <span class="math inline">\(f\)</span> the mean, <span class="math display">\[\begin{aligned}
g(M) &amp;= \sum_{m \in M} m\\
f(M) &amp;= \frac{\sum_{m \in M} m}{\sum_{m \in M}}\end{aligned}\]</span> then <span class="math display">\[f(\{g(A_{ij})\}) = \frac{\sum_{i, j} A_{ij}}{\sum_{i}}\]</span> is the average degree of the network described by <span class="math inline">\(A\)</span></p>
<h1 id="the-role-of-dynamics">The role of Dynamics</h1>
<p>Graphs encode a notion of locality. Functions that act covariantly can very easily be defined through local dynamics.</p>
<p>Linear dynamics on the graph seem to cover most cases of Graph Neural Networks considered so far.</p>
<h1 id="lifting-to-probabilities">Lifting to probabilities</h1>
<p>If we don’t just want to look at functions from graphs to nodes, but also look at functions of probability densities of graphs and nodes, an interesting potential connection to QFT arises. QFT studies linear state spaces on tensor towers like we looked at above and specifically bosonic and fermionic statistics concern the question of how the spaces considered transform under permutation.</p>
<p>However, this is a different permutation symmetry than we have above. In the space <span class="math inline">\(\mathbf{X}^{kn}\)</span> QFT considers permutations on the <span class="math inline">\(k\)</span> rather than the <span class="math inline">\(n\)</span>. So a direct application of, e.g. Fock spaces is not possible.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This can be said more abstractly of course but it should be relatively clear I hope<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
</body>
</html>
