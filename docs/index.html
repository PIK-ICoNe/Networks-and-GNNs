<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Frank Hellmann" />
  <title>Networks – Complex and Graph Neural</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Networks<br />
–<br />
Complex and Graph Neural</h1>
<p class="author">Frank Hellmann</p>
</header>
<h1 id="initial-observations">Initial observations</h1>
<p>We have a language clash: The graph in Graph Neural Networks (GNNs) is the network in Complex Networks. We will try not to use the word network in this Knowledge Repo. All Complex Networks and graphs we will consider are defined over an index set <span class="math inline">\(1 \dots n\)</span>.</p>
<h1 id="a-brief-taxonomy-of-neural-networks-under-permutations">A brief taxonomy of Neural Networks under Permutations</h1>
<p>Neural Networks (NN) are function approximators. That is, specifying a neural network requires specifying an input and output space first: <span class="math inline">\(f_\mathcal{NN}: \mathcal{X}\rightarrow \mathcal{Y}\)</span>. This space can carry extra structure. Set based neural networks deal with spaces that are product spaces <span class="math inline">\(\mathcal{X}= \mathbf{X}^n\)</span> and approximate functions that are permutation invariant or covariant. We will always use indices <span class="math inline">\(i, j\)</span> running from <span class="math inline">\(1 \dots n\)</span> to denote the copies. Writing <span class="math inline">\(S_n\)</span> for the symmetric group on the indices parametrizing the copies of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>, and <span class="math inline">\(P \in S_n\)</span> for the permutations we have the covariant <span class="math inline">\(f^C_\mathcal{NN}: \mathbf{X}^n \rightarrow \mathbf{Y}^n\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
f^C_\mathcal{NN}(P x) = P f^C_\mathcal{NN}(x)\end{aligned}\]</span></p>
<p>and the invariant <span class="math inline">\(f^I_\mathcal{NN}: \mathbf{X}^n \rightarrow \mathcal{Y}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
f^I_\mathcal{NN}(P x) = f^I_\mathcal{NN}(x)\end{aligned}\]</span></p>
<p>The name set based indicates that the functions learned only depend on the set of components <span class="math inline">\(x_i\)</span> in the vector <span class="math inline">\(x = (x_1, \dots, x_n)\)</span>, not on their order.</p>
<p>We can now consider more complex representations of the permutation group. For example, consider the space <span class="math inline">\(\mathcal{X}= \mathbf{X}^{n \times n}\)</span>. Elements of this space have components <span class="math inline">\(x_{ij}\)</span>. This space transforms naturally under <span class="math inline">\(S_n\)</span> by conjugation. A covariant network <span class="math inline">\(f^C_\mathcal{NN}: \mathbf{X}^{n \times n} \rightarrow \mathbf{Y}^n\)</span> would thus satisfy:</p>
<p><span class="math display">\[\begin{aligned}
f^C_\mathcal{NN}(P x P) = P f^C_\mathcal{NN}(x)\end{aligned}\]</span></p>
<p>Finally, for the space <span class="math inline">\(\mathcal{X}= \mathbf{X}_2^{n \times n} \oplus \mathbf{X}_1^{n}\)</span> we would obtain a covariant function</p>
<p><span class="math display">\[\begin{aligned}
f^C_\mathcal{NN}(P x_2 P, P x_1) = P f^C_\mathcal{NN}(x_2, x_1)\end{aligned}\]</span></p>
<p>This is the form of the symmetry of a Graph Neural Network. The coefficients <span class="math inline">\((x_2)_{ij}\)</span> contain the information on the connection from node <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>, in the simples case simply whether the nodes are connected or not.</p>
<h2 id="tensor-towers">Tensor towers</h2>
<p>More generally we can have an entire tower of tensors, where, at each point in the hierarchy, we take some base space <span class="math inline">\(\mathbf{X}_k\)</span>, take <span class="math inline">\(n\)</span> copies of it, and tensor it together <span class="math inline">\(k\)</span> times. <span class="math display">\[\begin{aligned}
\mathcal{X}= \bigoplus_{k=0}^{\infty} \bigotimes^k \mathbf{X}_k^n\end{aligned}\]</span></p>
<p>This also transforms naturally under <span class="math inline">\(S_n\)</span>. This space naturally carries data on hypergraphs. Covariant Neural networks on such spaces would appropriately be called Hypergraph Neural Networks.</p>
</body>
</html>
